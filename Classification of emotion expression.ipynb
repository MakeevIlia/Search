{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_XML(xml_file, df_cols): \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "\n",
    "    for node in xroot: \n",
    "        res = [\"NULL\",] * len(df_cols)\n",
    "        for col in node:\n",
    "            for idx, el in enumerate(df_cols): \n",
    "                if col.attrib.get(\"name\") == el:\n",
    "                    res[idx] = col.text\n",
    "        rows.append(res)\n",
    "\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    nrow, ncol = out_df.shape \n",
    "    target = [np.nan]*nrow\n",
    "    for i in range(nrow):\n",
    "        if out_df.loc[i,'beeline'] !='NULL':\n",
    "            target[i] = out_df.loc[i,'beeline']\n",
    "        elif out_df.loc[i,'mts'] !='NULL':\n",
    "            target[i] = out_df.loc[i,'mts']\n",
    "        elif out_df.loc[i,'megafon'] !='NULL':\n",
    "            target[i] = out_df.loc[i,'megafon']\n",
    "        elif out_df.loc[i,'tele2'] !='NULL':\n",
    "            target[i] = out_df.loc[i,'tele2']\n",
    "        elif out_df.loc[i,'rostelecom'] !='NULL':\n",
    "            target[i] = out_df.loc[i,'rostelecom']\n",
    "        elif out_df.loc[i,'komstar'] !='NULL':\n",
    "            target[i] = out_df.loc[i,'komstar']\n",
    "        elif out_df.loc[i,'skylink'] !='NULL':\n",
    "            target[i] = out_df.loc[i,'skylink']\n",
    "        else:\n",
    "            target[i] = 'NAN'\n",
    "\n",
    "    out_df = out_df.drop(columns = [\"beeline\",\"mts\",\"megafon\",\n",
    "                        \"tele2\",\"rostelecom\",\"komstar\",\"skylink\", \"id\", \"twitid\", \"date\"])\n",
    "    out_df['target'] = target\n",
    "\n",
    "    out_df = out_df.drop(out_df[out_df['target'].isin(['--','+-','NAN'])].index)\n",
    "    out_df['target'] = out_df['target'].apply(pd.to_numeric, errors='coerce')\n",
    "    return out_df\n",
    "\n",
    "\n",
    "a = parse_XML(\"C://Users//Frederik//Desktop//train.xml\", [\"id\", \"twitid\", \"date\", \"text\", \"beeline\", \"mts\", \"megafon\", \"tele2\", \"rostelecom\", \"komstar\",\"skylink\"])\n",
    "b = parse_XML(\"C://Users//Frederik//Desktop//test_etalon.xml\", [\"id\", \"twitid\", \"date\", \"text\", \"beeline\", \"mts\", \"megafon\", \"tele2\", \"rostelecom\", \"komstar\",\"skylink\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "import string\n",
    "\n",
    "\n",
    "def senttoterm(sents):\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        terms = \"\"\n",
    "        temp = sent.split()\n",
    "        for j in temp:\n",
    "            fl = True\n",
    "            while fl:\n",
    "                fl = False\n",
    "                for p in string.punctuation:\n",
    "                    if p in j:\n",
    "                        j = j.replace(p, '')\n",
    "                        fl = True\n",
    "            if j != \"\":\n",
    "                j = j.lower()\n",
    "                q = morph.parse(j)[0].normal_form\n",
    "                terms = terms + q + \" \" \n",
    "        res.append(terms)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7237971391417425"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4297352703595443\n",
      "0.7237971391417426\n",
      "0.6631867498170302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7347204161248374"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44612056379354836\n",
      "0.7347204161248374\n",
      "0.6780201481252234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7368010403120936"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46371162490258994\n",
      "0.7368010403120936\n",
      "0.6874216810497694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + frequencies + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7263979193758128"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43077481561825737\n",
      "0.7263979193758129\n",
      "0.665135345614521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + bool + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да], binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729518855656697"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4353722378747153\n",
      "0.729518855656697\n",
      "0.6692032415126267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + tf-idf + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7282184655396619"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7347204161248374"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44612056379354836\n",
      "0.7347204161248374\n",
      "0.6780201481252234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7368010403120936"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46371162490258994\n",
      "0.7368010403120936\n",
      "0.6874216810497694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + frequencies + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7263979193758128"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43077481561825737\n",
      "0.7263979193758129\n",
      "0.665135345614521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + bool + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"], binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729518855656697"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4353722378747153\n",
      "0.729518855656697\n",
      "0.6692032415126267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SVM + tf-idf + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "a[\"text\"] = senttoterm(a[\"text\"])\n",
    "b[\"text\"] = senttoterm(b[\"text\"])\n",
    "collection = list(a[\"text\"]) + list(b[\"text\"])\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, a[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7282184655396619"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(b[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4476818555803108\n",
      "0.7282184655396619\n",
      "0.6741142455338077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4476818555803108\n",
      "0.7282184655396619\n",
      "0.6741142455338077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    RidgeClassifier + frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5638268662164495\n",
      "0.6697009102730819\n",
      "0.6854478190511815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = RidgeClassifier().fit(Xtrain, a[\"target\"])\n",
    "\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], prved))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    RidgeClassifier + bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6736020806241872\n",
      "0.5651055599953573\n",
      "0.6736020806241872\n",
      "0.6887817145504974\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = RidgeClassifier().fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    RidgeClassifier + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7003901170351106\n",
      "0.5825209081961783\n",
      "0.7003901170351106\n",
      "0.7080310337938136\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = RidgeClassifier().fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    RidgeClassifier + frequencies + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6660598179453836\n",
      "0.5601947779400697\n",
      "0.6660598179453836\n",
      "0.6816936711188714\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = RidgeClassifier().fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    RidgeClassifier + bool + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6676202860858258\n",
      "0.5616735142347299\n",
      "0.6676202860858258\n",
      "0.6829790393569223\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"], binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = RidgeClassifier().fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                   RidgeClassifier + tf-idf + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7003901170351106\n",
      "0.5815393653880093\n",
      "0.7003901170351106\n",
      "0.7079427095366992\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = RidgeClassifier().fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                     Tree + frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5906371911573473\n",
      "0.4799496948239306\n",
      "0.5906371911573473\n",
      "0.6117509264028442\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                      Tree + bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5791937581274382\n",
      "0.46528366225292844\n",
      "0.5791937581274382\n",
      "0.6037415140175968\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    Tree + Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565409622886866\n",
      "0.4559360085401467\n",
      "0.565409622886866\n",
      "0.5870514778051633\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    Tree + frequencies + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5992197659297789\n",
      "0.49336183475132955\n",
      "0.5992197659297789\n",
      "0.6210901160643497\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        Tree + bool + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5950585175552666\n",
      "0.4837167418957206\n",
      "0.5950585175552666\n",
      "0.6168429078789522\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"], binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        Tree + TfIdf + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6007802340702211\n",
      "0.4865718468696327\n",
      "0.6007802340702211\n",
      "0.6210005076325182\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        XGBoost + frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6884265279583875\n",
      "0.3634710121900114\n",
      "0.6884265279583875\n",
      "0.6056726235213612\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor()\n",
    "reg.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = list(map(round,reg.predict(Xtest)))\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                      XGBoost + bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6889466840052015\n",
      "0.3703134082711124\n",
      "0.6889466840052015\n",
      "0.6096847564034826\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor()\n",
    "reg.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = list(map(round,reg.predict(Xtest)))\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    XGBoost + Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6918075422626788\n",
      "0.38032800081563334\n",
      "0.6918075422626788\n",
      "0.614498197665143\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor()\n",
    "reg.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = list(map(round,reg.predict(Xtest)))\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    XGBoost + frequencies + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6912873862158647\n",
      "0.3685233582159013\n",
      "0.6912873862158647\n",
      "0.6091184756719872\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor()\n",
    "reg.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = list(map(round,reg.predict(Xtest)))\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        XGBoost + bool + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694148244473342\n",
      "0.37454798917718035\n",
      "0.694148244473342\n",
      "0.6132775855846775\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"], binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor()\n",
    "reg.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = list(map(round,reg.predict(Xtest)))\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        XGBoost + TfIdf + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6918075422626788\n",
      "0.3724564775541279\n",
      "0.6918075422626788\n",
      "0.6114878169798192\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor()\n",
    "reg.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = list(map(round,reg.predict(Xtest)))\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                      SGD Classifier + frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313394018205461\n",
      "0.5088761553480652\n",
      "0.7313394018205462\n",
      "0.6981637951723694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                      SGD Classifier + bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071521456436931\n",
      "0.4099224065367462\n",
      "0.7071521456436931\n",
      "0.6370721681057432\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SGD Classifier + Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7396618985695709\n",
      "0.5245265791503186\n",
      "0.739661898569571\n",
      "0.7098653550314208\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    SGD Classifier + frequencies + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7006501950585176\n",
      "0.38215057504776556\n",
      "0.7006501950585176\n",
      "0.6188659060571302\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        SGD Classifier + bool + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368010403120936\n",
      "0.5285048014349959\n",
      "0.7368010403120936\n",
      "0.7107080634026807\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"], binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        SGD Classifier + TfIdf + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6918075422626788\n",
      "0.3500206022933296\n",
      "0.6918075422626788\n",
      "0.5954800117721268\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(Xtrain, a[\"target\"])\n",
    "\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                      MLP Classifier + frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-236-a226df2ff71d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[0;32m     91\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = MLPClassifier().fit(Xtrain, a[\"target\"])\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6291287386215865\n",
      "0.5378499041397239\n",
      "0.6291287386215865\n",
      "0.6567249513129668\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                      MLP Classifier + bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635110533159948\n",
      "0.5429267937379866\n",
      "0.635110533159948\n",
      "0.6623129023305587\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = MLPClassifier().fit(Xtrain, a[\"target\"])\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    MLP Classifier + Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6416124837451236\n",
      "0.5378496077121965\n",
      "0.6416124837451236\n",
      "0.6631861901769334\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = MLPClassifier().fit(Xtrain, a[\"target\"])\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    MLP Classifier + frequencies + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6338101430429128\n",
      "0.5399253812786872\n",
      "0.6338101430429128\n",
      "0.6608159627347614\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = MLPClassifier().fit(Xtrain, a[\"target\"])\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        MLP Classifier + bool + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6361508452535761\n",
      "0.5440000499112531\n",
      "0.6361508452535761\n",
      "0.6614095567913024\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"], binary = True)\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = MLPClassifier().fit(Xtrain, a[\"target\"])\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        MLP Classifier + TfIdf + stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6330299089726918\n",
      "0.5328338621843701\n",
      "0.6330299089726918\n",
      "0.6566076523353429\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = [\"он\", \"мы\", \"его\", \"вы\", \"вам\", \"вас\", \"ее\", \"что\", \"который\", \"их\", \"все\", \"они\", \"я\", \"весь\", \"мне\", \"меня\", \"таким\", \"для\", \"на\", \"по\", \"со\", \"из\", \"от\", \"до\", \"без\", \"над\", \"под\", \"за\", \"при\", \"после\", \"во\", \"же\", \"то\", \"бы\", \"всего\", \"итого\", \"даже\", \"да\"])\n",
    "lems = vectorizer.fit_transform(collection).toarray()\n",
    "Xtrain = lems[0:len(a[\"text\"])]\n",
    "Xtest = lems[len(a[\"text\"])::]\n",
    "\n",
    "clf = MLPClassifier().fit(Xtrain, a[\"target\"])\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(b[\"target\"], pred))\n",
    "print(f1_score(b[\"target\"], pred, average='macro'))\n",
    "print(f1_score(b[\"target\"], pred, average='micro'))\n",
    "print(f1_score(b[\"target\"], pred, average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
